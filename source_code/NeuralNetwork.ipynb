{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f16ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.21.5\n",
    "!pip install pandas==1.3.5\n",
    "!pip install scikit_learn==1.0.2\n",
    "!pip install torch==1.10.1\n",
    "!pip install matplotlib==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7776c2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8676e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "df = pd.read_csv(\"../Input/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77378ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>phone_no</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>multi_screen</th>\n",
       "      <th>mail_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>100198</td>\n",
       "      <td>409-8743</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>62</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>148.35</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16.81</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>100643</td>\n",
       "      <td>340-5930</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>149</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>294.45</td>\n",
       "      <td>7.7</td>\n",
       "      <td>33.37</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>100756</td>\n",
       "      <td>372-3750</td>\n",
       "      <td>Female</td>\n",
       "      <td>65</td>\n",
       "      <td>126</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>87.30</td>\n",
       "      <td>11.9</td>\n",
       "      <td>9.89</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>101595</td>\n",
       "      <td>331-4902</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>131</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>321.30</td>\n",
       "      <td>9.5</td>\n",
       "      <td>36.41</td>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>101653</td>\n",
       "      <td>351-8398</td>\n",
       "      <td>Female</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>243.00</td>\n",
       "      <td>10.9</td>\n",
       "      <td>27.54</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  customer_id  phone_no  gender  age  no_of_days_subscribed  \\\n",
       "0  2015       100198  409-8743  Female   36                     62   \n",
       "1  2015       100643  340-5930  Female   39                    149   \n",
       "2  2015       100756  372-3750  Female   65                    126   \n",
       "3  2015       101595  331-4902  Female   24                    131   \n",
       "4  2015       101653  351-8398  Female   40                    191   \n",
       "\n",
       "  multi_screen mail_subscribed  weekly_mins_watched  minimum_daily_mins  \\\n",
       "0           no              no               148.35                12.2   \n",
       "1           no              no               294.45                 7.7   \n",
       "2           no              no                87.30                11.9   \n",
       "3           no             yes               321.30                 9.5   \n",
       "4           no              no               243.00                10.9   \n",
       "\n",
       "   maximum_daily_mins  weekly_max_night_mins  videos_watched  \\\n",
       "0               16.81                     82               1   \n",
       "1               33.37                     87               3   \n",
       "2                9.89                     91               1   \n",
       "3               36.41                    102               4   \n",
       "4               27.54                     83               7   \n",
       "\n",
       "   maximum_days_inactive  customer_support_calls  churn  \n",
       "0                    4.0                       1    0.0  \n",
       "1                    3.0                       2    0.0  \n",
       "2                    4.0                       5    1.0  \n",
       "3                    3.0                       3    0.0  \n",
       "4                    3.0                       1    0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290beada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns\n",
    "data = df.drop([\"customer_id\", \"phone_no\", \"year\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c57431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>multi_screen</th>\n",
       "      <th>mail_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Female</td>\n",
       "      <td>54</td>\n",
       "      <td>75</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>182.25</td>\n",
       "      <td>11.3</td>\n",
       "      <td>20.66</td>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>127</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>273.45</td>\n",
       "      <td>9.3</td>\n",
       "      <td>30.99</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>94</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>128.85</td>\n",
       "      <td>15.6</td>\n",
       "      <td>14.60</td>\n",
       "      <td>110</td>\n",
       "      <td>16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>94</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>178.05</td>\n",
       "      <td>10.4</td>\n",
       "      <td>20.18</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Male</td>\n",
       "      <td>37</td>\n",
       "      <td>73</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>326.70</td>\n",
       "      <td>10.3</td>\n",
       "      <td>37.03</td>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  age  no_of_days_subscribed multi_screen mail_subscribed  \\\n",
       "1995  Female   54                     75           no             yes   \n",
       "1996    Male   45                    127           no              no   \n",
       "1997     NaN   53                     94           no              no   \n",
       "1998    Male   40                     94           no              no   \n",
       "1999    Male   37                     73           no              no   \n",
       "\n",
       "      weekly_mins_watched  minimum_daily_mins  maximum_daily_mins  \\\n",
       "1995               182.25                11.3               20.66   \n",
       "1996               273.45                 9.3               30.99   \n",
       "1997               128.85                15.6               14.60   \n",
       "1998               178.05                10.4               20.18   \n",
       "1999               326.70                10.3               37.03   \n",
       "\n",
       "      weekly_max_night_mins  videos_watched  maximum_days_inactive  \\\n",
       "1995                     97               5                    4.0   \n",
       "1996                    116               3                    3.0   \n",
       "1997                    110              16                    5.0   \n",
       "1998                    100               6                    NaN   \n",
       "1999                     89               6                    3.0   \n",
       "\n",
       "      customer_support_calls  churn  \n",
       "1995                       2    NaN  \n",
       "1996                       1    0.0  \n",
       "1997                       0    0.0  \n",
       "1998                       3    0.0  \n",
       "1999                       1    1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e4a959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.69050</td>\n",
       "      <td>99.750000</td>\n",
       "      <td>270.178425</td>\n",
       "      <td>10.198700</td>\n",
       "      <td>30.620780</td>\n",
       "      <td>100.415500</td>\n",
       "      <td>4.482500</td>\n",
       "      <td>3.250507</td>\n",
       "      <td>1.547000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.20641</td>\n",
       "      <td>39.755386</td>\n",
       "      <td>80.551627</td>\n",
       "      <td>2.785519</td>\n",
       "      <td>9.129165</td>\n",
       "      <td>19.529454</td>\n",
       "      <td>2.487728</td>\n",
       "      <td>0.809084</td>\n",
       "      <td>1.315164</td>\n",
       "      <td>0.340021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.00000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>218.212500</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>24.735000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.00000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>269.925000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>30.590000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44.00000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>324.675000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>36.797500</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82.00000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>526.200000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.640000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  no_of_days_subscribed  weekly_mins_watched  \\\n",
       "count  2000.00000            2000.000000          2000.000000   \n",
       "mean     38.69050              99.750000           270.178425   \n",
       "std      10.20641              39.755386            80.551627   \n",
       "min      18.00000               1.000000             0.000000   \n",
       "25%      32.00000              73.000000           218.212500   \n",
       "50%      37.00000              99.000000           269.925000   \n",
       "75%      44.00000             127.000000           324.675000   \n",
       "max      82.00000             243.000000           526.200000   \n",
       "\n",
       "       minimum_daily_mins  maximum_daily_mins  weekly_max_night_mins  \\\n",
       "count         2000.000000         2000.000000            2000.000000   \n",
       "mean            10.198700           30.620780             100.415500   \n",
       "std              2.785519            9.129165              19.529454   \n",
       "min              0.000000            0.000000              42.000000   \n",
       "25%              8.400000           24.735000              87.000000   \n",
       "50%             10.200000           30.590000             101.000000   \n",
       "75%             12.000000           36.797500             114.000000   \n",
       "max             20.000000           59.640000             175.000000   \n",
       "\n",
       "       videos_watched  maximum_days_inactive  customer_support_calls  \\\n",
       "count     2000.000000            1972.000000             2000.000000   \n",
       "mean         4.482500               3.250507                1.547000   \n",
       "std          2.487728               0.809084                1.315164   \n",
       "min          0.000000               0.000000                0.000000   \n",
       "25%          3.000000               3.000000                1.000000   \n",
       "50%          4.000000               3.000000                1.000000   \n",
       "75%          6.000000               4.000000                2.000000   \n",
       "max         19.000000               6.000000                9.000000   \n",
       "\n",
       "             churn  \n",
       "count  1965.000000  \n",
       "mean      0.133333  \n",
       "std       0.340021  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ec6e6",
   "metadata": {},
   "source": [
    "## Droping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5bd95ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                    24\n",
       "age                        0\n",
       "no_of_days_subscribed      0\n",
       "multi_screen               0\n",
       "mail_subscribed            0\n",
       "weekly_mins_watched        0\n",
       "minimum_daily_mins         0\n",
       "maximum_daily_mins         0\n",
       "weekly_max_night_mins      0\n",
       "videos_watched             0\n",
       "maximum_days_inactive     28\n",
       "customer_support_calls     0\n",
       "churn                     35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01bc1796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29c1dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping null values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9e96a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1918, 13)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the data after dropping null values\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae0d0a",
   "metadata": {},
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47534919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Female', 'Male'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique values in gender column\n",
    "data[\"gender\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6104bf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no' 'yes']\n",
      "['no' 'yes']\n"
     ]
    }
   ],
   "source": [
    "print(data[\"multi_screen\"].unique())\n",
    "print(data[\"mail_subscribed\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b27859ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2bce5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding categorical features\n",
    "data[\"gender\"] = le.fit_transform(data[\"gender\"])\n",
    "data[\"multi_screen\"] = le.fit_transform(data[\"multi_screen\"])\n",
    "data[\"mail_subscribed\"] = le.fit_transform(data[\"mail_subscribed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24f4f5b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>multi_screen</th>\n",
       "      <th>mail_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148.35</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16.81</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>294.45</td>\n",
       "      <td>7.7</td>\n",
       "      <td>33.37</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.30</td>\n",
       "      <td>11.9</td>\n",
       "      <td>9.89</td>\n",
       "      <td>91</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>321.30</td>\n",
       "      <td>9.5</td>\n",
       "      <td>36.41</td>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243.00</td>\n",
       "      <td>10.9</td>\n",
       "      <td>27.54</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  age  no_of_days_subscribed  multi_screen  mail_subscribed  \\\n",
       "0       0   36                     62             0                0   \n",
       "1       0   39                    149             0                0   \n",
       "2       0   65                    126             0                0   \n",
       "3       0   24                    131             0                1   \n",
       "4       0   40                    191             0                0   \n",
       "\n",
       "   weekly_mins_watched  minimum_daily_mins  maximum_daily_mins  \\\n",
       "0               148.35                12.2               16.81   \n",
       "1               294.45                 7.7               33.37   \n",
       "2                87.30                11.9                9.89   \n",
       "3               321.30                 9.5               36.41   \n",
       "4               243.00                10.9               27.54   \n",
       "\n",
       "   weekly_max_night_mins  videos_watched  maximum_days_inactive  \\\n",
       "0                     82               1                    4.0   \n",
       "1                     87               3                    3.0   \n",
       "2                     91               1                    4.0   \n",
       "3                    102               4                    3.0   \n",
       "4                     83               7                    3.0   \n",
       "\n",
       "   customer_support_calls  churn  \n",
       "0                       1    0.0  \n",
       "1                       2    0.0  \n",
       "2                       5    1.0  \n",
       "3                       3    0.0  \n",
       "4                       1    0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de53a22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churn\n",
       "0.0    1665\n",
       "1.0     253\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distribution of the target column\n",
    "data.groupby(\"churn\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32940e0e",
   "metadata": {},
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f45c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1d9ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping categorical columns and keeping numeircal columns only\n",
    "data_num = data.drop([\"gender\", \"multi_screen\", \"mail_subscribed\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1a4b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data_num.columns \n",
    "data_num = scaler.fit_transform(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6969db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of numerical columns only\n",
    "cols = list(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "132b1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols] = data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d000fc80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>no_of_days_subscribed</th>\n",
       "      <th>multi_screen</th>\n",
       "      <th>mail_subscribed</th>\n",
       "      <th>weekly_mins_watched</th>\n",
       "      <th>minimum_daily_mins</th>\n",
       "      <th>maximum_daily_mins</th>\n",
       "      <th>weekly_max_night_mins</th>\n",
       "      <th>videos_watched</th>\n",
       "      <th>maximum_days_inactive</th>\n",
       "      <th>customer_support_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.252066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.281927</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.281858</td>\n",
       "      <td>0.300752</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.611570</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.559578</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.338346</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.516529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165906</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.165828</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.537190</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610604</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.610496</td>\n",
       "      <td>0.451128</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.785124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.461802</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.461771</td>\n",
       "      <td>0.308271</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender       age  no_of_days_subscribed  multi_screen  mail_subscribed  \\\n",
       "0       0  0.281250               0.252066             0                0   \n",
       "1       0  0.328125               0.611570             0                0   \n",
       "2       0  0.734375               0.516529             0                0   \n",
       "3       0  0.093750               0.537190             0                1   \n",
       "4       0  0.343750               0.785124             0                0   \n",
       "\n",
       "   weekly_mins_watched  minimum_daily_mins  maximum_daily_mins  \\\n",
       "0             0.281927               0.610            0.281858   \n",
       "1             0.559578               0.385            0.559524   \n",
       "2             0.165906               0.595            0.165828   \n",
       "3             0.610604               0.475            0.610496   \n",
       "4             0.461802               0.545            0.461771   \n",
       "\n",
       "   weekly_max_night_mins  videos_watched  maximum_days_inactive  \\\n",
       "0               0.300752        0.052632               0.666667   \n",
       "1               0.338346        0.157895               0.500000   \n",
       "2               0.368421        0.052632               0.666667   \n",
       "3               0.451128        0.210526               0.500000   \n",
       "4               0.308271        0.368421               0.500000   \n",
       "\n",
       "   customer_support_calls  churn  \n",
       "0                0.111111    0.0  \n",
       "1                0.222222    0.0  \n",
       "2                0.555556    1.0  \n",
       "3                0.333333    0.0  \n",
       "4                0.111111    0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaab4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining dependent and indenpendent variables\n",
    "X = data.drop(\"churn\", axis=1)\n",
    "Y = data[\"churn\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc4b30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a dataset into train and test sets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3067ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c92264",
   "metadata": {},
   "source": [
    "## Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e8ce747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1918, 12)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of the training data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65b55055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for our network\n",
    "input_size= X.shape[1]\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b2e2d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6e843de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "model_dict = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d546a63",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the loss\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e914627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "521f2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "\n",
    "#converting data into tensor\n",
    "X_train = Tensor(X_train)\n",
    "y_train = Tensor(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0e77828",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# EPOCH = 200\n",
    "\n",
    "torch_dataset = Data.TensorDataset(X_train, y_train)\n",
    "\n",
    "#loading data for the model\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90aaf4a",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3767f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.007764913209724923\n",
      "Training loss: -0.008279809856974317\n",
      "Training loss: -0.008807059372574907\n",
      "Training loss: -0.00934175080395118\n",
      "Training loss: -0.009877931380054319\n",
      "Training loss: -0.010406260345874274\n",
      "Training loss: -0.010907325432229073\n",
      "Training loss: -0.011363567592578575\n",
      "Training loss: -0.011759582523261397\n",
      "Training loss: -0.012091111095508044\n",
      "Training loss: -0.012360696686915033\n",
      "Training loss: -0.012576443158974082\n",
      "Training loss: -0.01274770758608951\n",
      "Training loss: -0.01288036491134176\n",
      "Training loss: -0.012988415543072538\n",
      "Training loss: -0.013073865018270162\n",
      "Training loss: -0.013144111540047099\n",
      "Training loss: -0.013199973487170143\n",
      "Training loss: -0.01324689357190586\n",
      "Training loss: -0.013286307279258539\n",
      "Training loss: -0.013320732691763587\n",
      "Training loss: -0.013347436194314173\n",
      "Training loss: -0.013372294556208664\n",
      "Training loss: -0.013392008967318778\n",
      "Training loss: -0.013411497510241094\n",
      "Training loss: -0.013425560320838022\n",
      "Training loss: -0.013439034156724714\n",
      "Training loss: -0.013453151792715239\n",
      "Training loss: -0.013461767228361525\n",
      "Training loss: -0.013473706458351603\n",
      "Training loss: -0.013480774038307358\n",
      "Training loss: -0.013488637344156437\n",
      "Training loss: -0.013495763285374548\n",
      "Training loss: -0.013502498493667065\n",
      "Training loss: -0.013507754504292077\n",
      "Training loss: -0.013513770066153117\n",
      "Training loss: -0.01351966653529171\n",
      "Training loss: -0.013522910520026394\n",
      "Training loss: -0.013526457462658938\n",
      "Training loss: -0.013530325236811632\n",
      "Training loss: -0.013534882544227808\n",
      "Training loss: -0.013537554456482033\n",
      "Training loss: -0.013541954009753629\n",
      "Training loss: -0.013543868158134053\n",
      "Training loss: -0.01354591387191683\n",
      "Training loss: -0.01354878352083159\n",
      "Training loss: -0.013549520108348543\n",
      "Training loss: -0.01355370393477021\n",
      "Training loss: -0.013557094599774795\n",
      "Training loss: -0.013557115542997437\n",
      "Training loss: -0.013559963549286944\n",
      "Training loss: -0.013561720992626702\n",
      "Training loss: -0.01356274013879868\n",
      "Training loss: -0.013564641853354901\n",
      "Training loss: -0.013566130648380463\n",
      "Training loss: -0.013567882457643631\n",
      "Training loss: -0.013570206067397355\n",
      "Training loss: -0.013569538060192024\n",
      "Training loss: -0.013572057036408545\n",
      "Training loss: -0.013572248672724082\n",
      "Training loss: -0.013574341207626123\n",
      "Training loss: -0.013574420084698415\n",
      "Training loss: -0.013576738643211057\n",
      "Training loss: -0.013576416451741385\n",
      "Training loss: -0.013579303546865729\n",
      "Training loss: -0.01357889179300609\n",
      "Training loss: -0.01357941968655493\n",
      "Training loss: -0.013580560101368593\n",
      "Training loss: -0.013581030061067782\n",
      "Training loss: -0.013582439396185546\n",
      "Training loss: -0.013582187261544119\n",
      "Training loss: -0.013583211225822976\n",
      "Training loss: -0.013584218093593553\n",
      "Training loss: -0.013583878961038402\n",
      "Training loss: -0.013584837142616083\n",
      "Training loss: -0.013586417675950847\n",
      "Training loss: -0.013586997325292933\n",
      "Training loss: -0.013586590583818677\n",
      "Training loss: -0.013588758965048362\n",
      "Training loss: -0.013588965016828833\n",
      "Training loss: -0.013589150125386662\n",
      "Training loss: -0.013588323314932833\n",
      "Training loss: -0.013590112581091413\n",
      "Training loss: -0.01359122766198826\n",
      "Training loss: -0.013590372525728665\n",
      "Training loss: -0.013591132193531797\n",
      "Training loss: -0.013591541732616499\n",
      "Training loss: -0.013590969737847439\n",
      "Training loss: -0.013592351135716444\n",
      "Training loss: -0.013590437065047423\n",
      "Training loss: -0.01359343730797202\n",
      "Training loss: -0.01359281468422503\n",
      "Training loss: -0.013594801337342841\n",
      "Training loss: -0.01359317771303607\n",
      "Training loss: -0.013596128881210765\n",
      "Training loss: -0.013595142179548818\n",
      "Training loss: -0.013594804290376088\n",
      "Training loss: -0.013596088743272007\n",
      "Training loss: -0.013596392361716777\n",
      "Training loss: -0.013595699176017433\n",
      "Training loss: -0.013596636880640878\n",
      "Training loss: -0.013597559509252476\n",
      "Training loss: -0.013596525520202856\n",
      "Training loss: -0.013597111075611438\n",
      "Training loss: -0.013597370048856175\n",
      "Training loss: -0.013599254161780008\n",
      "Training loss: -0.013599174429882304\n",
      "Training loss: -0.013597448304237257\n",
      "Training loss: -0.013598986601425429\n",
      "Training loss: -0.013597248041956179\n",
      "Training loss: -0.013600419172252173\n",
      "Training loss: -0.013600630819253536\n",
      "Training loss: -0.01359855332150764\n",
      "Training loss: -0.013600722363284233\n",
      "Training loss: -0.013601255424641258\n",
      "Training loss: -0.013599484925786083\n",
      "Training loss: -0.01359935025192768\n",
      "Training loss: -0.013598555536282576\n",
      "Training loss: -0.013600379150880517\n",
      "Training loss: -0.01360023204319791\n",
      "Training loss: -0.01360172006110946\n",
      "Training loss: -0.013601567125071759\n",
      "Training loss: -0.013600424262348957\n",
      "Training loss: -0.013599940081463332\n",
      "Training loss: -0.013600430712395261\n",
      "Training loss: -0.013601900506983214\n",
      "Training loss: -0.013602384610157435\n",
      "Training loss: -0.013602536924503928\n",
      "Training loss: -0.013600393449778352\n",
      "Training loss: -0.01360185201506882\n",
      "Training loss: -0.013602320537107087\n",
      "Training loss: -0.01360345116028419\n",
      "Training loss: -0.013602603212329213\n",
      "Training loss: -0.01360339804454142\n",
      "Training loss: -0.013602217180943395\n",
      "Training loss: -0.013603990360841913\n",
      "Training loss: -0.013603136973088847\n",
      "Training loss: -0.013603264497498335\n",
      "Training loss: -0.01360338634897553\n",
      "Training loss: -0.013602523791277113\n",
      "Training loss: -0.01360363265526186\n",
      "Training loss: -0.013602767805077106\n",
      "Training loss: -0.013605176703093726\n",
      "Training loss: -0.013603981968010576\n",
      "Training loss: -0.013604749640088037\n",
      "Training loss: -0.013604198394263117\n",
      "Training loss: -0.013603653093360394\n",
      "Training loss: -0.013604414898227061\n",
      "Training loss: -0.013604846546205423\n",
      "Training loss: -0.013604297476299739\n",
      "Training loss: -0.013604726093533453\n",
      "Training loss: -0.013604824748157367\n",
      "Training loss: -0.013604596548627532\n",
      "Training loss: -0.013605675765712694\n",
      "Training loss: -0.013604787096983451\n",
      "Training loss: -0.013604221591116397\n",
      "Training loss: -0.013605627468076803\n",
      "Training loss: -0.01360473693427393\n",
      "Training loss: -0.013605807369970748\n",
      "Training loss: -0.013606222076863633\n",
      "Training loss: -0.013606308219951943\n",
      "Training loss: -0.013606721022915498\n",
      "Training loss: -0.013604508812455497\n",
      "Training loss: -0.013605573652931422\n",
      "Training loss: -0.013606967212634727\n",
      "Training loss: -0.013605735331501768\n",
      "Training loss: -0.013607128813493671\n",
      "Training loss: -0.013605238211667833\n",
      "Training loss: -0.01360629582498344\n",
      "Training loss: -0.013607354409691564\n",
      "Training loss: -0.013604478699287504\n",
      "Training loss: -0.013606521110335727\n",
      "Training loss: -0.01360659221626789\n",
      "Training loss: -0.013607976178613137\n",
      "Training loss: -0.01360542196027614\n",
      "Training loss: -0.01360745998063019\n",
      "Training loss: -0.013607199142311822\n",
      "Training loss: -0.01360660927392047\n",
      "Training loss: -0.01360733533154255\n",
      "Training loss: -0.013608384202325515\n",
      "Training loss: -0.013607463788488853\n",
      "Training loss: -0.013607201551365262\n",
      "Training loss: -0.013606934884691798\n",
      "Training loss: -0.013606667635182824\n",
      "Training loss: -0.013606730969974862\n",
      "Training loss: -0.013608757955309931\n",
      "Training loss: -0.013607506218913948\n",
      "Training loss: -0.013607568504602067\n",
      "Training loss: -0.013607295077036691\n",
      "Training loss: -0.013608337886330358\n",
      "Training loss: -0.013607408457971147\n",
      "Training loss: -0.01360844994617099\n",
      "Training loss: -0.01360719113803749\n",
      "Training loss: -0.013606918021317723\n",
      "Training loss: -0.01360795678961852\n",
      "Training loss: -0.013607025923598387\n",
      "Training loss: -0.013606094552454146\n",
      "Training loss: -0.013609426623062174\n",
      "Training loss: -0.013609476436070389\n",
      "Training loss: -0.013607560927740443\n",
      "Training loss: -0.013608595111068567\n",
      "Training loss: -0.013607988690148743\n",
      "Training loss: -0.013607707957711648\n",
      "Training loss: -0.013608085557410428\n",
      "Training loss: -0.013608789583850249\n",
      "Training loss: -0.013609492872031758\n",
      "Training loss: -0.013608552097807963\n",
      "Training loss: -0.013608600356588152\n",
      "Training loss: -0.013607332883633411\n",
      "Training loss: -0.013608034617586895\n",
      "Training loss: -0.01360676333677349\n",
      "Training loss: -0.013608121265799312\n",
      "Training loss: -0.013607838707144288\n",
      "Training loss: -0.013609849334229391\n",
      "Training loss: -0.013608909375975309\n",
      "Training loss: -0.013609607651771436\n",
      "Training loss: -0.013608991866627758\n",
      "Training loss: -0.013609360257525488\n",
      "Training loss: -0.01361071461182685\n",
      "Training loss: -0.013609112979846641\n",
      "Training loss: -0.013609481137610166\n",
      "Training loss: -0.013607551213815284\n",
      "Training loss: -0.01360890214881499\n",
      "Training loss: -0.01360992572453684\n",
      "Training loss: -0.013608324247979434\n",
      "Training loss: -0.013610001998277188\n",
      "Training loss: -0.013609381395026633\n",
      "Training loss: -0.013609746327767005\n",
      "Training loss: -0.013610112892446805\n",
      "Training loss: -0.013610148251134383\n",
      "Training loss: -0.0136088721910698\n",
      "Training loss: -0.013609562967715705\n",
      "Training loss: -0.013609597005309462\n",
      "Training loss: -0.01361028871449218\n",
      "Training loss: -0.013610321430992117\n",
      "Training loss: -0.013610027526472506\n",
      "Training loss: -0.013608423096881851\n",
      "Training loss: -0.013610752263000767\n",
      "Training loss: -0.013609144375252754\n",
      "Training loss: -0.01361147412420718\n",
      "Training loss: -0.013609866042180665\n",
      "Training loss: -0.013609570272587425\n",
      "Training loss: -0.013608944618095786\n",
      "Training loss: -0.013608318691614243\n",
      "Training loss: -0.013609662088608027\n",
      "Training loss: -0.01360936406538415\n",
      "Training loss: -0.013611364473419985\n",
      "Training loss: -0.013610080991916582\n",
      "Training loss: -0.013610440912271572\n",
      "Training loss: -0.0136098131984278\n",
      "Training loss: -0.013609842923038786\n",
      "Training loss: -0.013612170340651172\n",
      "Training loss: -0.01361055755708488\n",
      "Training loss: -0.01361091382500401\n",
      "Training loss: -0.01361094125712866\n",
      "Training loss: -0.013611955429770955\n",
      "Training loss: -0.013610341635956447\n",
      "Training loss: -0.013609382871543257\n",
      "Training loss: -0.0136107238206279\n",
      "Training loss: -0.01360976493964761\n",
      "Training loss: -0.013611106121866457\n",
      "Training loss: -0.013611789593640643\n",
      "Training loss: -0.013611157838804004\n",
      "Training loss: -0.01361052759933969\n",
      "Training loss: -0.013609896505049963\n",
      "Training loss: -0.013610578150606218\n",
      "Training loss: -0.01361060402850284\n",
      "Training loss: -0.01361030091518218\n",
      "Training loss: -0.01361163793984106\n",
      "Training loss: -0.01361166300176797\n",
      "Training loss: -0.01361135806222938\n",
      "Training loss: -0.013610726113114239\n",
      "Training loss: -0.013610095640515721\n",
      "Training loss: -0.013611757965100325\n",
      "Training loss: -0.013610469082654533\n",
      "Training loss: -0.013612133660869772\n",
      "Training loss: -0.01361150101235202\n",
      "Training loss: -0.013610866848461942\n",
      "Training loss: -0.013611219075388205\n",
      "Training loss: -0.013609928017023178\n",
      "Training loss: -0.013611591701557303\n",
      "Training loss: -0.013611940975450319\n",
      "Training loss: -0.013611964638572007\n",
      "Training loss: -0.01361100062863923\n",
      "Training loss: -0.013609707899479077\n",
      "Training loss: -0.01361038639772358\n",
      "Training loss: -0.013611064973679483\n",
      "Training loss: -0.013609770845714107\n",
      "Training loss: -0.013611107753805885\n",
      "Training loss: -0.01361112772563601\n",
      "Training loss: -0.013611476338982116\n",
      "Training loss: -0.013611497282204758\n",
      "Training loss: -0.013610861058962547\n",
      "Training loss: -0.01361055320524641\n",
      "Training loss: -0.013609916865437097\n",
      "Training loss: -0.013611576547834056\n",
      "Training loss: -0.013612582249933614\n",
      "Training loss: -0.01361194408390637\n",
      "Training loss: -0.013610321936116224\n",
      "Training loss: -0.013610342685060364\n",
      "Training loss: -0.013610362074054982\n",
      "Training loss: -0.013611363929440175\n",
      "Training loss: -0.01361171188223937\n",
      "Training loss: -0.013612385484665594\n",
      "Training loss: -0.013611092133814229\n",
      "Training loss: -0.01361143748328148\n",
      "Training loss: -0.01361211209595592\n",
      "Training loss: -0.013612130125001015\n",
      "Training loss: -0.013611163278602094\n",
      "Training loss: -0.013611837269585323\n",
      "Training loss: -0.013612183862434993\n",
      "Training loss: -0.013611214529271231\n",
      "Training loss: -0.013611562365503324\n",
      "Training loss: -0.013612235268526935\n",
      "Training loss: -0.013611268694117919\n",
      "Training loss: -0.013611284469532376\n",
      "Training loss: -0.013611628497905806\n",
      "Training loss: -0.013611646798940806\n",
      "Training loss: -0.013609692978890033\n",
      "Training loss: -0.013612663963472051\n",
      "Training loss: -0.013611368048144444\n",
      "Training loss: -0.013613682177107213\n",
      "Training loss: -0.013612055366632991\n",
      "Training loss: -0.013611743471923987\n",
      "Training loss: -0.013612416841216007\n",
      "Training loss: -0.013611444438451893\n",
      "Training loss: -0.01361146374973511\n",
      "Training loss: -0.013611806418159017\n",
      "Training loss: -0.01361116436656171\n",
      "Training loss: -0.013612823310700357\n",
      "Training loss: -0.013611195800823525\n",
      "Training loss: -0.013612195713423689\n",
      "Training loss: -0.013612210672868432\n",
      "Training loss: -0.013612882565643828\n",
      "Training loss: -0.013611911989097645\n",
      "Training loss: -0.013611269587799032\n",
      "Training loss: -0.01361161217851154\n",
      "Training loss: -0.013611627759647494\n",
      "Training loss: -0.013612955031525514\n",
      "Training loss: -0.013612312902216806\n",
      "Training loss: -0.013611341276566706\n",
      "Training loss: -0.013612340567475659\n",
      "Training loss: -0.013611698205032746\n",
      "Training loss: -0.013611711377115263\n",
      "Training loss: -0.013612054434096176\n",
      "Training loss: -0.01361272558861326\n",
      "Training loss: -0.01361142458318887\n",
      "Training loss: -0.013612095154870442\n",
      "Training loss: -0.01361243661876763\n",
      "Training loss: -0.013610806350136052\n",
      "Training loss: -0.013612462380097152\n",
      "Training loss: -0.01361346015563378\n",
      "Training loss: -0.01361183233491134\n",
      "Training loss: -0.013612173721097128\n",
      "Training loss: -0.013612187165169548\n",
      "Training loss: -0.01361088685914777\n",
      "Training loss: -0.013610899370683374\n",
      "Training loss: -0.013611895281146372\n",
      "Training loss: -0.013610595557960103\n",
      "Training loss: -0.01361060830262991\n",
      "Training loss: -0.013611605845032335\n",
      "Training loss: -0.013611617696021028\n",
      "Training loss: -0.01361031669059664\n",
      "Training loss: -0.013612298758741774\n",
      "Training loss: -0.013611655735751951\n",
      "Training loss: -0.013612322810420468\n",
      "Training loss: -0.013613648566926163\n",
      "Training loss: -0.013612675348192337\n",
      "Training loss: -0.01361268716032533\n",
      "Training loss: -0.013612370136663842\n",
      "Training loss: -0.013613039348395894\n",
      "Training loss: -0.013612720459660775\n",
      "Training loss: -0.013612733165474882\n",
      "Training loss: -0.013612415908679192\n",
      "Training loss: -0.013612100128400124\n",
      "Training loss: -0.013613752816770968\n",
      "Training loss: -0.01361146495426183\n",
      "Training loss: -0.013612788884549594\n",
      "Training loss: -0.01361279960872297\n",
      "Training loss: -0.013612156080609039\n",
      "Training loss: -0.01361183715301822\n",
      "Training loss: -0.013610862263489266\n",
      "Training loss: -0.013612843127107683\n",
      "Training loss: -0.013612854667250771\n",
      "Training loss: -0.01361417879181704\n",
      "Training loss: -0.013612219337689675\n",
      "Training loss: -0.01361288520783147\n",
      "Training loss: -0.013612895077179433\n",
      "Training loss: -0.013614220600550923\n",
      "Training loss: -0.013613573575423936\n",
      "Training loss: -0.013612599618431798\n",
      "Training loss: -0.013612281312532188\n",
      "Training loss: -0.013611633782281092\n",
      "Training loss: -0.013611643923618959\n",
      "Training loss: -0.013611982395627198\n",
      "Training loss: -0.013613963803225422\n",
      "Training loss: -0.013612660583026095\n",
      "Training loss: -0.013611028449320887\n",
      "Training loss: -0.013612021717596241\n",
      "Training loss: -0.013613017084079429\n",
      "Training loss: -0.013611714213581408\n",
      "Training loss: -0.013612708336682176\n",
      "Training loss: -0.013613374906226584\n",
      "Training loss: -0.013613384348161839\n",
      "Training loss: -0.013614049946313732\n",
      "Training loss: -0.013612089248803945\n",
      "Training loss: -0.013613412324266296\n",
      "Training loss: -0.013614078116696692\n",
      "Training loss: -0.013613102566620829\n",
      "Training loss: -0.013613112902237197\n",
      "Training loss: -0.013612792614696856\n",
      "Training loss: -0.013612474114518745\n",
      "Training loss: -0.013612810838020455\n",
      "Training loss: -0.013614461156193561\n",
      "Training loss: -0.01361282894477695\n",
      "Training loss: -0.013612837842732395\n",
      "Training loss: -0.013614160102225034\n",
      "Training loss: -0.01361384090264431\n",
      "Training loss: -0.013612864186897427\n",
      "Training loss: -0.013613528852512503\n",
      "Training loss: -0.013613537905890752\n",
      "Training loss: -0.013612890220216852\n",
      "Training loss: -0.013612898729615292\n",
      "Training loss: -0.013613564327767185\n",
      "Training loss: -0.013613244856196556\n",
      "Training loss: -0.01361325297703799\n",
      "Training loss: -0.013612933155766055\n",
      "Training loss: -0.013612283099894418\n",
      "Training loss: -0.013612292230984067\n",
      "Training loss: -0.013613286509507638\n",
      "Training loss: -0.013613293076121045\n",
      "Training loss: -0.013613302129499294\n",
      "Training loss: -0.01361396741680558\n",
      "Training loss: -0.01361331782720235\n",
      "Training loss: -0.013612012314516687\n",
      "Training loss: -0.013612020007945412\n",
      "Training loss: -0.013612685100973196\n",
      "Training loss: -0.013612693299526031\n",
      "Training loss: -0.013612373011985689\n",
      "Training loss: -0.013614351544262066\n",
      "Training loss: -0.01361337339085426\n",
      "Training loss: -0.013612725316623355\n",
      "Training loss: -0.013613716331268072\n",
      "Training loss: -0.01361405352103819\n",
      "Training loss: -0.013613732922652243\n",
      "Training loss: -0.013612754264120328\n",
      "Training loss: -0.013612105257352607\n",
      "Training loss: -0.013613755342391511\n",
      "Training loss: -0.01361179130329147\n",
      "Training loss: -0.013612784066442715\n",
      "Training loss: -0.013610821775849205\n",
      "Training loss: -0.013614440990084931\n",
      "Training loss: -0.013612478272078711\n",
      "Training loss: -0.013611499458123994\n",
      "Training loss: -0.013613148921471686\n",
      "Training loss: -0.013613813237385457\n",
      "Training loss: -0.013614148367803442\n",
      "Training loss: -0.013614812683717213\n",
      "Training loss: -0.013614162783268378\n",
      "Training loss: -0.013613184319014966\n",
      "Training loss: -0.01361384941204275\n",
      "Training loss: -0.013612214441871395\n",
      "Training loss: -0.013613205728506016\n",
      "Training loss: -0.013613870432976793\n",
      "Training loss: -0.013612891036186566\n",
      "Training loss: -0.013613555934935847\n",
      "Training loss: -0.013612905373940101\n",
      "Training loss: -0.013612584153862944\n",
      "Training loss: -0.013612918429455514\n",
      "Training loss: -0.013613910765194054\n",
      "Training loss: -0.013612275561888494\n",
      "Training loss: -0.013613266304543308\n",
      "Training loss: -0.013614915379333993\n",
      "Training loss: -0.013613280098317034\n",
      "Training loss: -0.013614271812364362\n",
      "Training loss: -0.01361230707386171\n",
      "Training loss: -0.01361362766255922\n",
      "Training loss: -0.013612649392584312\n",
      "Training loss: -0.013613312581682765\n",
      "Training loss: -0.013612990467924493\n",
      "Training loss: -0.01361398202654902\n",
      "Training loss: -0.013613988942863734\n",
      "Training loss: -0.013613337604753974\n",
      "Training loss: -0.013614001260120835\n",
      "Training loss: -0.013613022174176212\n",
      "Training loss: -0.013614670394141484\n",
      "Training loss: -0.01361270612190724\n",
      "Training loss: -0.013611398588725144\n",
      "Training loss: -0.01361337521707219\n",
      "Training loss: -0.013612067683890092\n",
      "Training loss: -0.013613716486690875\n",
      "Training loss: -0.013615035676583162\n",
      "Training loss: -0.013613399152183781\n",
      "Training loss: -0.013612747775218322\n",
      "Training loss: -0.0136130825170793\n",
      "Training loss: -0.013612102731732066\n",
      "Training loss: -0.013612765843119119\n",
      "Training loss: -0.013613100196423089\n",
      "Training loss: -0.013614420124573689\n",
      "Training loss: -0.013613111270297771\n",
      "Training loss: -0.013613774537107624\n",
      "Training loss: -0.013613124014967578\n",
      "Training loss: -0.013614771380107436\n",
      "Training loss: -0.01361444887779216\n",
      "Training loss: -0.013612155653196333\n",
      "Training loss: -0.013613475192789926\n",
      "Training loss: -0.013612823854680167\n",
      "Training loss: -0.013611515311249853\n",
      "Training loss: -0.013613819648576062\n",
      "Training loss: -0.013613169204147419\n",
      "Training loss: -0.013613174061109998\n",
      "Training loss: -0.013613837055929947\n",
      "Training loss: -0.013614826866047945\n",
      "Training loss: -0.013615162074177331\n",
      "Training loss: -0.013612867567343383\n",
      "Training loss: -0.0136125447930382\n",
      "Training loss: -0.013612879496043477\n",
      "Training loss: -0.013611241805973078\n",
      "Training loss: -0.013614203154341338\n",
      "Training loss: -0.013614208516428026\n",
      "Training loss: -0.013613555779513044\n",
      "Training loss: -0.013613890599085424\n",
      "Training loss: -0.013613237978737544\n",
      "Training loss: -0.013613244079082544\n",
      "Training loss: -0.013612593556942498\n",
      "Training loss: -0.013614239756411337\n",
      "Training loss: -0.01361326028190971\n",
      "Training loss: -0.013613593586109763\n",
      "Training loss: -0.013613597860236833\n",
      "Training loss: -0.013612617297775588\n",
      "Training loss: -0.013612294173769099\n",
      "Training loss: -0.01361427041355914\n",
      "Training loss: -0.013613618842315177\n",
      "Training loss: -0.013610667402150578\n",
      "Training loss: -0.013614284984446877\n",
      "Training loss: -0.013613961977007491\n",
      "Training loss: -0.01361396745566128\n",
      "Training loss: -0.013614956333242464\n",
      "Training loss: -0.013614634025205689\n",
      "Training loss: -0.013613324160681554\n",
      "Training loss: -0.013613000492695258\n",
      "Training loss: -0.013614320148855953\n",
      "Training loss: -0.013612682653064057\n",
      "Training loss: -0.013613015801841308\n",
      "Training loss: -0.013612363375771932\n",
      "Training loss: -0.013614995966057112\n",
      "Training loss: -0.013613029245913727\n",
      "Training loss: -0.013614020182847044\n",
      "Training loss: -0.013614024340407012\n",
      "Training loss: -0.013612715019862685\n",
      "Training loss: -0.013615019357188896\n",
      "Training loss: -0.013614366076294104\n",
      "Training loss: -0.013612728852492113\n",
      "Training loss: -0.01361404738183749\n",
      "Training loss: -0.013614052433078573\n",
      "Training loss: -0.013614384571607607\n",
      "Training loss: -0.013613732339816734\n",
      "Training loss: -0.013613737080212211\n",
      "Training loss: -0.01361538386251656\n",
      "Training loss: -0.013614731009034476\n",
      "Training loss: -0.01361309413493379\n",
      "Training loss: -0.013613426351174227\n",
      "Training loss: -0.013612773769682047\n",
      "Training loss: -0.013612449751994445\n",
      "Training loss: -0.013614097427979909\n",
      "Training loss: -0.013614101624395577\n",
      "Training loss: -0.013613448149222284\n",
      "Training loss: -0.013613781336855235\n",
      "Training loss: -0.013615100021623415\n",
      "Training loss: -0.013614775965080112\n",
      "Training loss: -0.013614779734083074\n",
      "Training loss: -0.013612813091651091\n",
      "Training loss: -0.013613802318933579\n",
      "Training loss: -0.013613150281421208\n",
      "Training loss: -0.013611512202793804\n",
      "Training loss: -0.013612172905127414\n",
      "Training loss: -0.013614147007853919\n",
      "Training loss: -0.013613824544394342\n",
      "Training loss: -0.013614484935882349\n",
      "Training loss: -0.01361251844887317\n",
      "Training loss: -0.013614163754660893\n",
      "Training loss: -0.013612526258868997\n",
      "Training loss: -0.013613188010306526\n",
      "Training loss: -0.013613848090948929\n",
      "Training loss: -0.013613195198611143\n",
      "Training loss: -0.01361385733860568\n",
      "Training loss: -0.013614188738876402\n",
      "Training loss: -0.013613535535693014\n",
      "Training loss: -0.013612883770170548\n",
      "Training loss: -0.013613543811957248\n",
      "Training loss: -0.013614204941703567\n",
      "Training loss: -0.013613223524416908\n",
      "Training loss: -0.013613227176852767\n",
      "Training loss: -0.013612902615185355\n",
      "Training loss: -0.013614549008932698\n",
      "Training loss: -0.01361488161373014\n",
      "Training loss: -0.01361357081666919\n",
      "Training loss: -0.013614889423725969\n",
      "Training loss: -0.013614564667780054\n",
      "Training loss: -0.013613253870719103\n",
      "Training loss: -0.013614243719692802\n",
      "Training loss: -0.013615561199934262\n",
      "Training loss: -0.013613595684317598\n",
      "Training loss: -0.01361524087353822\n",
      "Training loss: -0.013614915029632687\n",
      "Training loss: -0.01361426314754312\n",
      "Training loss: -0.01361328052572974\n",
      "Training loss: -0.013614269986146432\n",
      "Training loss: -0.013614930805047146\n",
      "Training loss: -0.013614606204524034\n",
      "Training loss: -0.013614280826886909\n",
      "Training loss: -0.013612970768084272\n",
      "Training loss: -0.013614616656707506\n",
      "Training loss: -0.013612977800966087\n",
      "Training loss: -0.013613310328052127\n",
      "Training loss: -0.013613641922601354\n",
      "Training loss: -0.013613316661531332\n",
      "Training loss: -0.013612992488420926\n",
      "Training loss: -0.013612666955361\n",
      "Training loss: -0.013612670840931063\n",
      "Training loss: -0.013614644943657567\n",
      "Training loss: -0.013612021484462037\n",
      "Training loss: -0.013614652637086293\n",
      "Training loss: -0.013614327997707482\n",
      "Training loss: -0.013614002542358955\n",
      "Training loss: -0.013612692250422113\n",
      "Training loss: -0.013614665692601707\n",
      "Training loss: -0.01361335563379907\n",
      "Training loss: -0.01361368812202941\n",
      "Training loss: -0.013614676455630784\n",
      "Training loss: -0.013613366785385151\n",
      "Training loss: -0.013614355041275124\n",
      "Training loss: -0.01361173080496558\n",
      "Training loss: -0.013613704324856575\n",
      "Training loss: -0.013614036929654018\n",
      "Training loss: -0.013613054929531848\n",
      "Training loss: -0.013613386135524068\n",
      "Training loss: -0.013614703266064222\n",
      "Training loss: -0.013613393673529992\n",
      "Training loss: -0.013614381463151557\n",
      "Training loss: -0.013613400007009195\n",
      "Training loss: -0.01361504527394122\n",
      "Training loss: -0.013613077426982517\n",
      "Training loss: -0.013615051762843226\n",
      "Training loss: -0.013614726734907406\n",
      "Training loss: -0.013613087607176083\n",
      "Training loss: -0.013614075202519145\n",
      "Training loss: -0.013614407885027989\n",
      "Training loss: -0.013614410643782734\n",
      "Training loss: -0.013613428915650469\n",
      "Training loss: -0.013612118274012321\n",
      "Training loss: -0.013613763579800046\n",
      "Training loss: -0.013614424048999454\n",
      "Training loss: -0.013614098554795227\n",
      "Training loss: -0.013614429916210249\n",
      "Training loss: -0.013613776169047052\n",
      "Training loss: -0.013615750038639352\n",
      "Training loss: -0.013615753535652409\n",
      "Training loss: -0.013613785727549408\n",
      "Training loss: -0.013614116739263124\n",
      "Training loss: -0.013613134506006751\n",
      "Training loss: -0.013612480253719444\n",
      "Training loss: -0.0136124836730211\n",
      "Training loss: -0.013614785717860971\n",
      "Training loss: -0.013613146939830954\n",
      "Training loss: -0.013612492415553743\n",
      "Training loss: -0.013614466090867541\n",
      "Training loss: -0.01361479842367508\n",
      "Training loss: -0.013614800483027214\n",
      "Training loss: -0.013613161549574393\n",
      "Training loss: -0.013612835355967556\n",
      "Training loss: -0.013613495980589765\n",
      "Training loss: -0.013614485246727955\n",
      "Training loss: -0.01361350173123346\n",
      "Training loss: -0.013613176509019139\n",
      "Training loss: -0.013613836278815935\n",
      "Training loss: -0.013615481662315061\n",
      "Training loss: -0.01361449958448149\n",
      "Training loss: -0.013614501954679227\n",
      "Training loss: -0.013612862127545294\n",
      "Training loss: -0.013615164832932075\n",
      "Training loss: -0.013613853181045712\n",
      "Training loss: -0.0136135285805226\n",
      "Training loss: -0.013615830236805465\n",
      "Training loss: -0.013614190642805734\n",
      "Training loss: -0.013613207982136654\n",
      "Training loss: -0.013614524141284291\n",
      "Training loss: -0.013614856318669027\n",
      "Training loss: -0.013616172672095169\n",
      "Training loss: -0.013615190633117297\n",
      "Training loss: -0.013614207583891211\n",
      "Training loss: -0.01361256857272699\n",
      "Training loss: -0.013613884421029022\n",
      "Training loss: -0.013614543996547317\n",
      "Training loss: -0.013614546910724864\n",
      "Training loss: -0.013614220872540828\n",
      "Training loss: -0.013614552311667251\n",
      "Training loss: -0.013615211148927234\n",
      "Training loss: -0.013614557829176742\n",
      "Training loss: -0.013615216705292425\n",
      "Training loss: -0.013614563268974832\n",
      "Training loss: -0.013612923364129495\n",
      "Training loss: -0.013614568320215913\n",
      "Training loss: -0.013612271054627222\n",
      "Training loss: -0.013613588340590177\n",
      "Training loss: -0.013613590594220815\n",
      "Training loss: -0.013613593003274254\n",
      "Training loss: -0.013614909822968802\n",
      "Training loss: -0.013614255298691591\n",
      "Training loss: -0.013614257396899426\n",
      "Training loss: -0.013614917943810236\n",
      "Training loss: -0.013614591245079289\n",
      "Training loss: -0.013614594742092347\n",
      "Training loss: -0.013614597306568589\n",
      "Training loss: -0.013613943054281282\n",
      "Training loss: -0.013614273871716496\n",
      "Training loss: -0.013612961986695928\n",
      "Training loss: -0.013614278806390476\n",
      "Training loss: -0.013614610012382697\n",
      "Training loss: -0.013615598695685375\n",
      "Training loss: -0.013614615063623778\n",
      "Training loss: -0.013612974614798634\n",
      "Training loss: -0.013614619260039448\n",
      "Training loss: -0.013614293921258024\n",
      "Training loss: -0.013614296330311464\n",
      "Training loss: -0.013614627536303683\n",
      "Training loss: -0.013614630178491327\n",
      "Training loss: -0.013614960646225234\n",
      "Training loss: -0.013614963016422973\n",
      "Training loss: -0.013615294261270894\n",
      "Training loss: -0.013615953370520779\n",
      "Training loss: -0.013615299273656276\n",
      "Training loss: -0.013615301294152709\n",
      "Training loss: -0.013614975916515584\n",
      "Training loss: -0.013613992711866694\n",
      "Training loss: -0.013612681215403133\n",
      "Training loss: -0.013613012110549747\n",
      "Training loss: -0.013614656444944957\n",
      "Training loss: -0.013613345065048497\n",
      "Training loss: -0.013613347046689228\n",
      "Training loss: -0.013614663050414064\n",
      "Training loss: -0.013615323014489365\n",
      "Training loss: -0.013614996704315424\n",
      "Training loss: -0.013614342335461016\n",
      "Training loss: -0.01361533008622688\n",
      "Training loss: -0.013614675717372472\n",
      "Training loss: -0.0136146773881676\n",
      "Training loss: -0.013614680185778045\n",
      "Training loss: -0.013614353953315506\n",
      "Training loss: -0.013614355896100537\n",
      "Training loss: -0.013615015704753035\n",
      "Training loss: -0.01361271836145294\n",
      "Training loss: -0.013613049062321053\n",
      "Training loss: -0.013614364949478786\n",
      "Training loss: -0.013615352544821848\n",
      "Training loss: -0.013615354837308184\n",
      "Training loss: -0.013613715515298358\n",
      "Training loss: -0.01361503120817759\n",
      "Training loss: -0.013615033539519627\n",
      "Training loss: -0.01361240840952897\n",
      "Training loss: -0.013614381618574358\n",
      "Training loss: -0.013615040028421633\n",
      "Training loss: -0.013614713873670496\n",
      "Training loss: -0.013614058766557776\n",
      "Training loss: -0.013615047294437653\n",
      "Training loss: -0.013614063778943157\n",
      "Training loss: -0.013613409293521648\n",
      "Training loss: -0.01361505389990676\n",
      "Training loss: -0.013613741509762084\n",
      "Training loss: -0.013613743297124312\n",
      "Training loss: -0.013615717127860914\n",
      "Training loss: -0.013613090832199236\n",
      "Training loss: -0.013613750330006127\n",
      "Training loss: -0.013614738275050496\n",
      "Training loss: -0.013615068665073001\n",
      "Training loss: -0.013615399365941113\n",
      "Training loss: -0.013615401425293248\n",
      "Training loss: -0.013613104043137453\n",
      "Training loss: -0.013615734340936296\n",
      "Training loss: -0.013615078689843766\n",
      "Training loss: -0.013614424243277957\n",
      "Training loss: -0.013615083896507651\n",
      "Training loss: -0.013614757858323615\n",
      "Training loss: -0.013615087121530804\n",
      "Training loss: -0.013614761705037979\n",
      "Training loss: -0.013614435045162732\n",
      "Training loss: -0.013614436288545154\n",
      "Training loss: -0.013614766795134762\n",
      "Training loss: -0.013615098350828287\n",
      "Training loss: -0.013614443166004166\n",
      "Training loss: -0.013614444953366396\n",
      "Training loss: -0.013614118759759557\n",
      "Training loss: -0.013614448955503561\n",
      "Training loss: -0.01361609325104307\n",
      "Training loss: -0.01361478198771371\n",
      "Training loss: -0.013613469597569034\n",
      "Training loss: -0.013615114709078256\n",
      "Training loss: -0.01361380235778928\n",
      "Training loss: -0.013613475697914034\n",
      "Training loss: -0.0136144636429584\n",
      "Training loss: -0.013613807175896159\n",
      "Training loss: -0.013614138964723887\n",
      "Training loss: -0.013614140713230416\n",
      "Training loss: -0.01361479900651059\n",
      "Training loss: -0.013614800988151321\n",
      "Training loss: -0.013614803669194665\n",
      "Training loss: -0.013613819881710267\n",
      "Training loss: -0.013613821163948388\n",
      "Training loss: -0.013614809342126959\n",
      "Training loss: -0.013614154040735734\n",
      "Training loss: -0.013614156372077774\n",
      "Training loss: -0.013614814743069347\n",
      "Training loss: -0.013614160024513633\n",
      "Training loss: -0.013613833675483992\n",
      "Training loss: -0.013614492512743973\n",
      "Training loss: -0.013614165891724428\n",
      "Training loss: -0.013615481817737862\n",
      "Training loss: -0.013615154652738508\n",
      "Training loss: -0.01361515655666784\n",
      "Training loss: -0.013615487218680252\n",
      "Training loss: -0.013614175217092581\n",
      "Training loss: -0.013614176693609205\n",
      "Training loss: -0.013613850461146666\n",
      "Training loss: -0.013613852170797495\n",
      "Training loss: -0.013615167669398222\n",
      "Training loss: -0.013614512834275406\n",
      "Training loss: -0.013614187068081275\n",
      "Training loss: -0.013614517224969577\n",
      "Training loss: -0.013614518507207699\n",
      "Training loss: -0.013613534680867598\n",
      "Training loss: -0.013614850917726639\n",
      "Training loss: -0.013614852627377466\n",
      "Training loss: -0.013613540897779702\n",
      "Training loss: -0.01361551344627818\n",
      "Training loss: -0.013615515117073308\n",
      "Training loss: -0.013613217385216207\n",
      "Training loss: -0.013614204825136466\n",
      "Training loss: -0.013612892085290483\n",
      "Training loss: -0.013614208361005223\n",
      "Training loss: -0.013614209682099046\n",
      "Training loss: -0.0136138829833681\n",
      "Training loss: -0.013614213489957708\n",
      "Training loss: -0.013613558071999382\n",
      "Training loss: -0.013615530970199166\n",
      "Training loss: -0.013614875357962339\n",
      "Training loss: -0.01361520605883045\n",
      "Training loss: -0.013613893979531378\n",
      "Training loss: -0.013613566814532026\n",
      "Training loss: -0.01361488211885425\n",
      "Training loss: -0.013614884722186192\n",
      "Training loss: -0.013615214762507393\n",
      "Training loss: -0.013612916564381884\n",
      "Training loss: -0.01361521825952045\n",
      "Training loss: -0.013613905985942875\n",
      "Training loss: -0.013613579092933425\n",
      "Training loss: -0.01361390932753313\n",
      "Training loss: -0.013614568164793112\n",
      "Training loss: -0.013615882963991228\n",
      "Training loss: -0.013613914611908416\n",
      "Training loss: -0.013614573177178494\n",
      "Training loss: -0.013612931990095037\n",
      "Training loss: -0.013614904888294822\n",
      "Training loss: -0.013614578072996774\n",
      "Training loss: -0.013614250985708821\n",
      "Training loss: -0.013614909667546\n",
      "Training loss: -0.013614583707073366\n",
      "Training loss: -0.0136142558426714\n",
      "Training loss: -0.013614915068488389\n",
      "Training loss: -0.013614916545005013\n",
      "Training loss: -0.013615903674079666\n",
      "Training loss: -0.013614591517069194\n",
      "Training loss: -0.013614264468636941\n",
      "Training loss: -0.013614594314679639\n",
      "Training loss: -0.013614596374031774\n",
      "Training loss: -0.013614926608631478\n",
      "Training loss: -0.013613613713362692\n",
      "Training loss: -0.013613944258808002\n",
      "Training loss: -0.013615259329996021\n",
      "Training loss: -0.013615918089544602\n",
      "Training loss: -0.013614277640719458\n",
      "Training loss: -0.013615921780836163\n",
      "Training loss: -0.013614609429547187\n",
      "Training loss: -0.013614939392156987\n",
      "Training loss: -0.013613626846589507\n",
      "Training loss: -0.013615928114315366\n",
      "Training loss: -0.01361461572417069\n",
      "Training loss: -0.013613631586984984\n",
      "Training loss: -0.013614947085585713\n",
      "Training loss: -0.01361494879523654\n",
      "Training loss: -0.01361265044168823\n",
      "Training loss: -0.01361560813762063\n",
      "Training loss: -0.013614296252600062\n",
      "Training loss: -0.013614626331776963\n",
      "Training loss: -0.013615285207892645\n",
      "Training loss: -0.01361430107070694\n",
      "Training loss: -0.013615288899184206\n",
      "Training loss: -0.013613975615358416\n",
      "Training loss: -0.013615619677763719\n",
      "Training loss: -0.01361660727310678\n",
      "Training loss: -0.01361398020033109\n",
      "Training loss: -0.01361431020179659\n",
      "Training loss: -0.013614311872591718\n",
      "Training loss: -0.013614970515573196\n",
      "Training loss: -0.013614643156295339\n",
      "Training loss: -0.013614315835873182\n",
      "Training loss: -0.013615631761886616\n",
      "Training loss: -0.013614319682587545\n",
      "Training loss: -0.013614649528630243\n",
      "Training loss: -0.013615307394497709\n",
      "Training loss: -0.013614324150993118\n",
      "Training loss: -0.013615639416459642\n",
      "Training loss: -0.013613998073953382\n",
      "Training loss: -0.013615313455987007\n",
      "Training loss: -0.013614986679544661\n",
      "Training loss: -0.013614330834173629\n",
      "Training loss: -0.013615647032176966\n",
      "Training loss: -0.01361466281727986\n",
      "Training loss: -0.01361532095513723\n",
      "Training loss: -0.013614665614890307\n",
      "Training loss: -0.013614995499788705\n",
      "Training loss: -0.013615325112697198\n",
      "Training loss: -0.013615326550358122\n",
      "Training loss: -0.013614342996007927\n",
      "Training loss: -0.01361434435595745\n",
      "Training loss: -0.013614345910185474\n",
      "Training loss: -0.01361533261184742\n",
      "Training loss: -0.01361500564112657\n",
      "Training loss: -0.01361369297899199\n",
      "Training loss: -0.013615007933612908\n",
      "Training loss: -0.013615338284779715\n",
      "Training loss: -0.01361468294453279\n",
      "Training loss: -0.013614355585254933\n",
      "Training loss: -0.01361501407281361\n",
      "Training loss: -0.013615015316196029\n",
      "Training loss: -0.013614688112340975\n",
      "Training loss: -0.013613375372494992\n",
      "Training loss: -0.013615676717932252\n",
      "Training loss: -0.013614035608560197\n",
      "Training loss: -0.013615021805098035\n",
      "Training loss: -0.013615023631315965\n",
      "Training loss: -0.013614039338707458\n",
      "Training loss: -0.013614368873904551\n",
      "Training loss: -0.013613385047564451\n",
      "Training loss: -0.013614371943504901\n",
      "Training loss: -0.01361535880058965\n",
      "Training loss: -0.013613717730073294\n",
      "Training loss: -0.013615361753622898\n",
      "Training loss: -0.013613720605395141\n",
      "Training loss: -0.013614050179447935\n",
      "Training loss: -0.01361470855043951\n",
      "Training loss: -0.013614053054769782\n",
      "Training loss: -0.01361668242003181\n",
      "Training loss: -0.01361405534725612\n",
      "Training loss: -0.013614385309865919\n",
      "Training loss: -0.013615372477796274\n",
      "Training loss: -0.01361274579357759\n",
      "Training loss: -0.013614389933694295\n",
      "Training loss: -0.013614719430035687\n",
      "Training loss: -0.013614720673418108\n",
      "Training loss: -0.013616036832565745\n",
      "Training loss: -0.013615051840554627\n",
      "Training loss: -0.0136143963448849\n",
      "Training loss: -0.013616039785598994\n",
      "Training loss: -0.013616041184404216\n",
      "Training loss: -0.013613743258268612\n",
      "Training loss: -0.013616701031912414\n",
      "Training loss: -0.013615388292066433\n",
      "Training loss: -0.013614404077169327\n",
      "Training loss: -0.013614405203984645\n",
      "Training loss: -0.013615063885821824\n",
      "Training loss: -0.013615393498730317\n",
      "Training loss: -0.013615394470122834\n",
      "Training loss: -0.013614410527215633\n",
      "Training loss: -0.013612769495554978\n",
      "Training loss: -0.013615069947311122\n",
      "Training loss: -0.013613757052042338\n",
      "Training loss: -0.013614744297684094\n",
      "Training loss: -0.013614416938406237\n",
      "Training loss: -0.013613104120848853\n",
      "Training loss: -0.013615076008800422\n",
      "Training loss: -0.013615078145863957\n",
      "Training loss: -0.013615735545463015\n",
      "Training loss: -0.013614751719122915\n",
      "Training loss: -0.013615409740413184\n",
      "Training loss: -0.013615411100362705\n",
      "Training loss: -0.01361475506071317\n",
      "Training loss: -0.013613442864846996\n",
      "Training loss: -0.013611800706371024\n",
      "Training loss: -0.013615744249139958\n",
      "Training loss: -0.013613774692530427\n",
      "Training loss: -0.01361508988028555\n",
      "Training loss: -0.01361509108481227\n",
      "Training loss: -0.01361542116398917\n",
      "Training loss: -0.013615422640505795\n",
      "Training loss: -0.013615751631723079\n",
      "Training loss: -0.013613782541381955\n",
      "Training loss: -0.013614768660208392\n",
      "Training loss: -0.013614441456353338\n",
      "Training loss: -0.01361477157438594\n",
      "Training loss: -0.0136154291294078\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y.type(torch.LongTensor))\n",
    "        \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(b_x)\n",
    "        loss = criterion(output, b_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f08eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with test data\n",
    "X_test = Tensor(X_test)\n",
    "y_test = Tensor(np.array(y_test))\n",
    "z = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "733342bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9961e-01, 3.8929e-04],\n",
       "        [9.9977e-01, 2.2851e-04],\n",
       "        [9.9995e-01, 4.7851e-05],\n",
       "        [9.9993e-01, 6.5534e-05],\n",
       "        [9.9996e-01, 4.2665e-05],\n",
       "        [9.9999e-01, 1.1407e-05],\n",
       "        [9.9991e-01, 9.4185e-05],\n",
       "        [9.9999e-01, 7.7333e-06],\n",
       "        [9.9991e-01, 8.9144e-05],\n",
       "        [9.9988e-01, 1.1666e-04],\n",
       "        [9.9982e-01, 1.7669e-04],\n",
       "        [9.9987e-01, 1.2996e-04],\n",
       "        [9.9997e-01, 3.0559e-05],\n",
       "        [9.9989e-01, 1.1265e-04],\n",
       "        [9.9993e-01, 6.9915e-05],\n",
       "        [9.9996e-01, 4.4919e-05],\n",
       "        [9.9994e-01, 5.8609e-05],\n",
       "        [9.9998e-01, 2.1187e-05],\n",
       "        [9.9995e-01, 5.3770e-05],\n",
       "        [9.9998e-01, 2.3366e-05],\n",
       "        [9.9999e-01, 7.6403e-06],\n",
       "        [9.9996e-01, 4.2651e-05],\n",
       "        [9.9998e-01, 1.6911e-05],\n",
       "        [9.9981e-01, 1.8880e-04],\n",
       "        [9.9999e-01, 1.4817e-05],\n",
       "        [9.9993e-01, 6.7087e-05],\n",
       "        [9.9977e-01, 2.2741e-04],\n",
       "        [9.9997e-01, 2.7352e-05],\n",
       "        [9.9994e-01, 6.0926e-05],\n",
       "        [9.9995e-01, 4.8970e-05],\n",
       "        [9.9994e-01, 5.9607e-05],\n",
       "        [9.9995e-01, 4.5261e-05],\n",
       "        [9.9997e-01, 2.8361e-05],\n",
       "        [9.9994e-01, 6.1246e-05],\n",
       "        [9.9998e-01, 1.7513e-05],\n",
       "        [9.9999e-01, 1.4047e-05],\n",
       "        [9.9981e-01, 1.8898e-04],\n",
       "        [9.9999e-01, 1.3280e-05],\n",
       "        [9.9994e-01, 6.4240e-05],\n",
       "        [9.9998e-01, 1.5324e-05],\n",
       "        [9.9999e-01, 7.9634e-06],\n",
       "        [9.9960e-01, 4.0068e-04],\n",
       "        [9.9996e-01, 3.7868e-05],\n",
       "        [9.9983e-01, 1.7463e-04],\n",
       "        [9.9981e-01, 1.9325e-04],\n",
       "        [9.9996e-01, 3.5463e-05],\n",
       "        [9.9996e-01, 4.4979e-05],\n",
       "        [9.9996e-01, 4.0964e-05],\n",
       "        [9.9999e-01, 5.5333e-06],\n",
       "        [9.9967e-01, 3.2939e-04],\n",
       "        [9.9995e-01, 4.7454e-05],\n",
       "        [9.9993e-01, 7.2882e-05],\n",
       "        [9.9998e-01, 2.4678e-05],\n",
       "        [9.9974e-01, 2.6186e-04],\n",
       "        [9.9997e-01, 2.5403e-05],\n",
       "        [9.9997e-01, 2.9216e-05],\n",
       "        [9.9996e-01, 4.3459e-05],\n",
       "        [9.9997e-01, 2.8970e-05],\n",
       "        [9.9986e-01, 1.4181e-04],\n",
       "        [9.9971e-01, 2.8787e-04],\n",
       "        [9.9996e-01, 3.7923e-05],\n",
       "        [9.9997e-01, 3.4741e-05],\n",
       "        [9.9994e-01, 5.8286e-05],\n",
       "        [9.9982e-01, 1.7786e-04],\n",
       "        [9.9998e-01, 1.5350e-05],\n",
       "        [9.9998e-01, 1.7669e-05],\n",
       "        [9.9989e-01, 1.0658e-04],\n",
       "        [9.9990e-01, 1.0435e-04],\n",
       "        [9.9979e-01, 2.1019e-04],\n",
       "        [9.9992e-01, 8.4315e-05],\n",
       "        [9.9999e-01, 7.2653e-06],\n",
       "        [9.9989e-01, 1.1019e-04],\n",
       "        [9.9993e-01, 6.7439e-05],\n",
       "        [9.9999e-01, 1.4304e-05],\n",
       "        [9.9996e-01, 3.9015e-05],\n",
       "        [9.9994e-01, 6.2975e-05],\n",
       "        [9.9995e-01, 4.8691e-05],\n",
       "        [9.9980e-01, 1.9609e-04],\n",
       "        [9.9993e-01, 7.2108e-05],\n",
       "        [9.9988e-01, 1.2464e-04],\n",
       "        [9.9995e-01, 4.5734e-05],\n",
       "        [9.9991e-01, 9.3013e-05],\n",
       "        [9.9981e-01, 1.8633e-04],\n",
       "        [9.9999e-01, 1.0254e-05],\n",
       "        [9.9997e-01, 3.1009e-05],\n",
       "        [9.9986e-01, 1.3610e-04],\n",
       "        [9.9978e-01, 2.1956e-04],\n",
       "        [9.9992e-01, 8.4562e-05],\n",
       "        [9.9992e-01, 8.0185e-05],\n",
       "        [9.9999e-01, 7.8908e-06],\n",
       "        [9.9995e-01, 5.2043e-05],\n",
       "        [9.9999e-01, 1.1636e-05],\n",
       "        [9.9997e-01, 3.2350e-05],\n",
       "        [9.9935e-01, 6.4778e-04],\n",
       "        [9.9983e-01, 1.7477e-04],\n",
       "        [9.9996e-01, 4.2553e-05],\n",
       "        [9.9992e-01, 7.5332e-05],\n",
       "        [9.9995e-01, 5.1067e-05],\n",
       "        [9.9999e-01, 8.7415e-06],\n",
       "        [9.9977e-01, 2.3454e-04],\n",
       "        [9.9995e-01, 5.4327e-05],\n",
       "        [9.9996e-01, 4.1487e-05],\n",
       "        [9.9998e-01, 2.4037e-05],\n",
       "        [9.9997e-01, 2.7237e-05],\n",
       "        [9.9999e-01, 8.3775e-06],\n",
       "        [9.9997e-01, 3.2520e-05],\n",
       "        [9.9998e-01, 1.5786e-05],\n",
       "        [9.9986e-01, 1.4461e-04],\n",
       "        [9.9998e-01, 2.3863e-05],\n",
       "        [9.9991e-01, 8.5650e-05],\n",
       "        [9.9997e-01, 3.1985e-05],\n",
       "        [9.9992e-01, 8.0977e-05],\n",
       "        [9.9999e-01, 8.4728e-06],\n",
       "        [9.9995e-01, 5.4854e-05],\n",
       "        [9.9998e-01, 2.4463e-05],\n",
       "        [9.9989e-01, 1.1344e-04],\n",
       "        [9.9998e-01, 2.4213e-05],\n",
       "        [9.9996e-01, 4.0458e-05],\n",
       "        [9.9982e-01, 1.7991e-04],\n",
       "        [9.9985e-01, 1.5313e-04],\n",
       "        [9.9985e-01, 1.4933e-04],\n",
       "        [9.9944e-01, 5.6089e-04],\n",
       "        [9.9994e-01, 6.0204e-05],\n",
       "        [9.9999e-01, 1.3097e-05],\n",
       "        [9.9986e-01, 1.4369e-04],\n",
       "        [9.9995e-01, 4.6981e-05],\n",
       "        [9.9989e-01, 1.1466e-04],\n",
       "        [9.9990e-01, 9.8527e-05],\n",
       "        [9.9972e-01, 2.7794e-04],\n",
       "        [9.9996e-01, 4.1995e-05],\n",
       "        [9.9990e-01, 9.5767e-05],\n",
       "        [9.9993e-01, 6.5122e-05],\n",
       "        [9.9992e-01, 8.1826e-05],\n",
       "        [9.9999e-01, 1.4529e-05],\n",
       "        [9.9990e-01, 9.6802e-05],\n",
       "        [9.9991e-01, 8.8644e-05],\n",
       "        [9.9982e-01, 1.7792e-04],\n",
       "        [9.9986e-01, 1.4195e-04],\n",
       "        [9.9999e-01, 5.5472e-06],\n",
       "        [9.9997e-01, 2.7423e-05],\n",
       "        [9.9982e-01, 1.8085e-04],\n",
       "        [9.9992e-01, 7.8577e-05],\n",
       "        [9.9996e-01, 3.6682e-05],\n",
       "        [9.9997e-01, 2.6951e-05],\n",
       "        [9.9997e-01, 3.4566e-05],\n",
       "        [9.9995e-01, 5.2726e-05],\n",
       "        [9.9998e-01, 1.6762e-05],\n",
       "        [9.9980e-01, 2.0265e-04],\n",
       "        [9.9996e-01, 3.8729e-05],\n",
       "        [9.9991e-01, 9.3409e-05],\n",
       "        [9.9988e-01, 1.2216e-04],\n",
       "        [9.9998e-01, 1.8254e-05],\n",
       "        [9.9992e-01, 7.5818e-05],\n",
       "        [9.9993e-01, 7.3533e-05],\n",
       "        [9.9978e-01, 2.1856e-04],\n",
       "        [9.9986e-01, 1.4017e-04],\n",
       "        [9.9992e-01, 7.5745e-05],\n",
       "        [9.9962e-01, 3.7695e-04],\n",
       "        [9.9997e-01, 2.9677e-05],\n",
       "        [9.9997e-01, 3.2430e-05],\n",
       "        [9.9996e-01, 4.4070e-05],\n",
       "        [9.9999e-01, 1.3538e-05],\n",
       "        [9.9997e-01, 3.2329e-05],\n",
       "        [9.9994e-01, 5.5601e-05],\n",
       "        [9.9991e-01, 9.0641e-05],\n",
       "        [9.9996e-01, 4.0621e-05],\n",
       "        [9.9986e-01, 1.4498e-04],\n",
       "        [9.9996e-01, 4.2027e-05],\n",
       "        [9.9993e-01, 6.7012e-05],\n",
       "        [9.9996e-01, 3.5609e-05],\n",
       "        [9.9999e-01, 1.2177e-05],\n",
       "        [9.9995e-01, 5.0718e-05],\n",
       "        [9.9994e-01, 6.1574e-05],\n",
       "        [9.9997e-01, 2.8502e-05],\n",
       "        [9.9998e-01, 2.0939e-05],\n",
       "        [9.9993e-01, 7.2141e-05],\n",
       "        [9.9997e-01, 2.8592e-05],\n",
       "        [9.9991e-01, 9.1761e-05],\n",
       "        [9.9996e-01, 4.0957e-05],\n",
       "        [9.9997e-01, 2.5842e-05],\n",
       "        [9.9984e-01, 1.6417e-04],\n",
       "        [9.9996e-01, 4.4395e-05],\n",
       "        [9.9998e-01, 2.0217e-05],\n",
       "        [9.9996e-01, 4.3275e-05],\n",
       "        [9.9988e-01, 1.1894e-04],\n",
       "        [9.9998e-01, 2.0301e-05],\n",
       "        [9.9986e-01, 1.4248e-04],\n",
       "        [9.9983e-01, 1.6893e-04],\n",
       "        [9.9999e-01, 8.4630e-06],\n",
       "        [9.9998e-01, 1.8823e-05],\n",
       "        [9.9983e-01, 1.6858e-04],\n",
       "        [9.9999e-01, 1.0711e-05],\n",
       "        [9.9995e-01, 4.9624e-05],\n",
       "        [9.9999e-01, 1.2079e-05],\n",
       "        [9.9981e-01, 1.8792e-04],\n",
       "        [9.9998e-01, 2.3953e-05],\n",
       "        [9.9996e-01, 3.9974e-05],\n",
       "        [9.9999e-01, 6.7529e-06],\n",
       "        [9.9997e-01, 2.8172e-05],\n",
       "        [9.9998e-01, 2.4801e-05],\n",
       "        [9.9996e-01, 4.4508e-05],\n",
       "        [9.9999e-01, 1.2178e-05],\n",
       "        [9.9995e-01, 4.9225e-05],\n",
       "        [9.9996e-01, 4.3710e-05],\n",
       "        [9.9999e-01, 9.4341e-06],\n",
       "        [9.9997e-01, 3.3551e-05],\n",
       "        [9.9984e-01, 1.6020e-04],\n",
       "        [9.9995e-01, 5.4320e-05],\n",
       "        [9.9997e-01, 3.3220e-05],\n",
       "        [9.9999e-01, 1.1523e-05],\n",
       "        [9.9997e-01, 2.5407e-05],\n",
       "        [9.9999e-01, 1.3054e-05],\n",
       "        [9.9998e-01, 1.7259e-05],\n",
       "        [9.9999e-01, 9.4955e-06],\n",
       "        [9.9997e-01, 2.9845e-05],\n",
       "        [9.9996e-01, 3.8465e-05],\n",
       "        [9.9995e-01, 4.5473e-05],\n",
       "        [9.9991e-01, 8.7319e-05],\n",
       "        [9.9986e-01, 1.4022e-04],\n",
       "        [9.9995e-01, 4.6652e-05],\n",
       "        [9.9995e-01, 4.5428e-05],\n",
       "        [9.9996e-01, 3.8873e-05],\n",
       "        [9.9997e-01, 2.8812e-05],\n",
       "        [9.9997e-01, 2.7690e-05],\n",
       "        [9.9994e-01, 6.1332e-05],\n",
       "        [9.9999e-01, 8.0325e-06],\n",
       "        [9.9984e-01, 1.6221e-04],\n",
       "        [9.9999e-01, 8.3711e-06],\n",
       "        [9.9996e-01, 4.0897e-05],\n",
       "        [9.9999e-01, 1.4033e-05],\n",
       "        [9.9992e-01, 7.9800e-05],\n",
       "        [9.9978e-01, 2.2124e-04],\n",
       "        [9.9998e-01, 2.2216e-05],\n",
       "        [9.9975e-01, 2.5236e-04],\n",
       "        [9.9994e-01, 5.5548e-05],\n",
       "        [9.9997e-01, 3.4755e-05],\n",
       "        [9.9994e-01, 6.4129e-05],\n",
       "        [9.9994e-01, 6.3104e-05],\n",
       "        [9.9995e-01, 5.1846e-05],\n",
       "        [9.9998e-01, 1.8858e-05],\n",
       "        [9.9997e-01, 2.6090e-05],\n",
       "        [9.9995e-01, 5.3474e-05],\n",
       "        [9.9998e-01, 1.5029e-05],\n",
       "        [9.9996e-01, 4.0901e-05],\n",
       "        [9.9989e-01, 1.1310e-04],\n",
       "        [9.9988e-01, 1.1669e-04],\n",
       "        [9.9996e-01, 3.5701e-05],\n",
       "        [9.9974e-01, 2.5866e-04],\n",
       "        [9.9971e-01, 2.8871e-04],\n",
       "        [9.9996e-01, 3.7525e-05],\n",
       "        [9.9991e-01, 8.9141e-05],\n",
       "        [9.9983e-01, 1.6544e-04],\n",
       "        [9.9998e-01, 1.5391e-05],\n",
       "        [9.9997e-01, 3.1745e-05],\n",
       "        [9.9999e-01, 9.9180e-06],\n",
       "        [9.9998e-01, 2.3438e-05],\n",
       "        [9.9986e-01, 1.3732e-04],\n",
       "        [9.9989e-01, 1.0614e-04],\n",
       "        [9.9999e-01, 7.4236e-06],\n",
       "        [9.9998e-01, 1.5314e-05],\n",
       "        [9.9976e-01, 2.3506e-04],\n",
       "        [9.9997e-01, 3.4683e-05],\n",
       "        [9.9994e-01, 5.6026e-05],\n",
       "        [9.9996e-01, 4.4690e-05],\n",
       "        [9.9998e-01, 1.6188e-05],\n",
       "        [9.9968e-01, 3.2441e-04],\n",
       "        [9.9979e-01, 2.1414e-04],\n",
       "        [9.9996e-01, 3.5441e-05],\n",
       "        [9.9993e-01, 6.6169e-05],\n",
       "        [9.9998e-01, 2.1370e-05],\n",
       "        [9.9975e-01, 2.5338e-04],\n",
       "        [9.9986e-01, 1.4188e-04],\n",
       "        [9.9992e-01, 7.7339e-05],\n",
       "        [9.9991e-01, 9.4762e-05],\n",
       "        [9.9998e-01, 1.6609e-05],\n",
       "        [9.9995e-01, 4.5120e-05],\n",
       "        [9.9982e-01, 1.7988e-04],\n",
       "        [9.9998e-01, 2.2285e-05],\n",
       "        [9.9958e-01, 4.2310e-04],\n",
       "        [9.9990e-01, 9.6261e-05],\n",
       "        [9.9997e-01, 3.1269e-05],\n",
       "        [9.9997e-01, 2.7121e-05],\n",
       "        [9.9985e-01, 1.5384e-04],\n",
       "        [9.9987e-01, 1.3301e-04],\n",
       "        [9.9995e-01, 4.9280e-05],\n",
       "        [9.9999e-01, 9.7288e-06],\n",
       "        [9.9994e-01, 6.1949e-05],\n",
       "        [9.9997e-01, 3.4530e-05],\n",
       "        [9.9992e-01, 8.3726e-05],\n",
       "        [9.9994e-01, 6.1218e-05],\n",
       "        [9.9995e-01, 4.9703e-05],\n",
       "        [9.9997e-01, 3.4475e-05],\n",
       "        [9.9996e-01, 3.8837e-05],\n",
       "        [9.9996e-01, 3.9186e-05],\n",
       "        [9.9988e-01, 1.2067e-04],\n",
       "        [9.9999e-01, 1.3095e-05],\n",
       "        [9.9996e-01, 4.0786e-05],\n",
       "        [9.9982e-01, 1.7747e-04],\n",
       "        [9.9999e-01, 1.0588e-05],\n",
       "        [9.9998e-01, 1.9255e-05],\n",
       "        [9.9995e-01, 5.2690e-05],\n",
       "        [9.9998e-01, 1.5715e-05],\n",
       "        [9.9980e-01, 2.0459e-04],\n",
       "        [9.9999e-01, 1.1183e-05],\n",
       "        [9.9998e-01, 1.6095e-05],\n",
       "        [9.9966e-01, 3.4296e-04],\n",
       "        [9.9994e-01, 5.5406e-05],\n",
       "        [9.9998e-01, 2.1067e-05],\n",
       "        [9.9976e-01, 2.3974e-04],\n",
       "        [9.9999e-01, 1.0509e-05],\n",
       "        [9.9998e-01, 1.7439e-05],\n",
       "        [9.9995e-01, 5.2772e-05],\n",
       "        [9.9993e-01, 6.5604e-05],\n",
       "        [9.9999e-01, 1.1380e-05],\n",
       "        [9.9998e-01, 2.0510e-05],\n",
       "        [9.9999e-01, 8.1131e-06],\n",
       "        [1.0000e+00, 2.8917e-06],\n",
       "        [9.9994e-01, 5.5689e-05],\n",
       "        [9.9989e-01, 1.0881e-04],\n",
       "        [9.9995e-01, 4.5271e-05],\n",
       "        [9.9994e-01, 5.7031e-05],\n",
       "        [9.9993e-01, 6.8962e-05],\n",
       "        [9.9989e-01, 1.0935e-04],\n",
       "        [9.9991e-01, 9.2819e-05],\n",
       "        [9.9994e-01, 5.7255e-05],\n",
       "        [9.9987e-01, 1.3113e-04],\n",
       "        [9.9995e-01, 5.1730e-05],\n",
       "        [9.9969e-01, 3.0924e-04],\n",
       "        [9.9983e-01, 1.7071e-04],\n",
       "        [9.9999e-01, 7.6545e-06],\n",
       "        [9.9997e-01, 3.0909e-05],\n",
       "        [9.9984e-01, 1.6410e-04],\n",
       "        [9.9995e-01, 4.9606e-05],\n",
       "        [9.9997e-01, 2.6051e-05],\n",
       "        [9.9997e-01, 2.5173e-05],\n",
       "        [9.9995e-01, 5.3265e-05],\n",
       "        [9.9998e-01, 2.1168e-05],\n",
       "        [9.9992e-01, 8.2253e-05],\n",
       "        [9.9992e-01, 8.0326e-05],\n",
       "        [9.9996e-01, 3.9945e-05],\n",
       "        [9.9991e-01, 9.1960e-05],\n",
       "        [9.9994e-01, 6.0255e-05],\n",
       "        [9.9991e-01, 8.5293e-05],\n",
       "        [9.9989e-01, 1.0651e-04],\n",
       "        [9.9986e-01, 1.3799e-04],\n",
       "        [9.9999e-01, 5.0856e-06],\n",
       "        [9.9992e-01, 7.7282e-05],\n",
       "        [9.9999e-01, 1.1895e-05],\n",
       "        [9.9990e-01, 1.0174e-04],\n",
       "        [9.9983e-01, 1.7270e-04],\n",
       "        [9.9995e-01, 5.1250e-05],\n",
       "        [9.9997e-01, 3.1994e-05],\n",
       "        [9.9999e-01, 1.3941e-05],\n",
       "        [9.9997e-01, 2.7433e-05],\n",
       "        [9.9966e-01, 3.3532e-04],\n",
       "        [9.9998e-01, 1.8148e-05],\n",
       "        [9.9991e-01, 8.6226e-05],\n",
       "        [9.9999e-01, 1.0031e-05],\n",
       "        [9.9995e-01, 5.3368e-05],\n",
       "        [9.9993e-01, 7.1308e-05],\n",
       "        [9.9985e-01, 1.4532e-04],\n",
       "        [9.9998e-01, 2.1843e-05],\n",
       "        [9.9997e-01, 2.5194e-05],\n",
       "        [9.9995e-01, 4.5139e-05],\n",
       "        [9.9981e-01, 1.8952e-04],\n",
       "        [9.9997e-01, 2.9472e-05],\n",
       "        [9.9995e-01, 4.8513e-05],\n",
       "        [9.9992e-01, 7.6362e-05],\n",
       "        [9.9999e-01, 1.0339e-05],\n",
       "        [9.9995e-01, 5.3879e-05],\n",
       "        [9.9995e-01, 5.0773e-05],\n",
       "        [9.9998e-01, 1.8412e-05],\n",
       "        [9.9997e-01, 3.3728e-05],\n",
       "        [9.9987e-01, 1.2572e-04],\n",
       "        [9.9980e-01, 1.9988e-04],\n",
       "        [9.9997e-01, 2.9021e-05],\n",
       "        [9.9987e-01, 1.2691e-04],\n",
       "        [9.9998e-01, 1.9804e-05],\n",
       "        [9.9992e-01, 7.8617e-05],\n",
       "        [9.9996e-01, 3.6360e-05],\n",
       "        [9.9976e-01, 2.3778e-04],\n",
       "        [9.9996e-01, 3.7792e-05],\n",
       "        [9.9997e-01, 2.4993e-05],\n",
       "        [9.9998e-01, 2.2620e-05]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f96dbbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting prediction by the model\n",
    "yhat = list(z.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e11801bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting tensor into list\n",
    "y_test = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8351d8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.859375"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
